#!/bin/bash
#
# s3-functions

bucket-object-count() {
  # Get object count for S3 buckets from CloudWatch metrics
  #
  #     $ bucket-object-count my-bucket
  #     my-bucket  1234567  2025-06-30T00:00:00.000Z
  #
  #     $ buckets | bucket-object-count
  #     bucket-one    45678   2025-06-30T00:00:00.000Z
  #     bucket-two    123456  2025-06-30T00:00:00.000Z
  #     bucket-three  NO_DATA NO_DATA
  #
  # Note: CloudWatch metrics are updated daily. Shows NO_DATA if metrics unavailable.
  # For version counts, use bucket-version-count function.

  local bucket_names=$(skim-stdin "$@")
  [[ -z "$bucket_names" ]] && __bma_usage "bucket-name [bucket-name]" && return 1

  local bucket_name
  for bucket_name in $bucket_names; do
    # Get the NumberOfObjects metric from CloudWatch with timestamp
    local metric_data=$(
      aws cloudwatch get-metric-statistics \
        --namespace AWS/S3 \
        --metric-name NumberOfObjects \
        --dimensions Name=BucketName,Value="$bucket_name" Name=StorageType,Value=AllStorageTypes \
        --statistics Average \
        --start-time $(date -u -d '2 days ago' +%Y-%m-%dT%H:%M:%S) \
        --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
        --period 86400 \
        --output text \
        --query 'Datapoints | sort_by(@, &Timestamp) | [-1].[Average, Timestamp] || [`NO_DATA`, `NO_DATA`]'
    )
    
    # Parse the output
    local object_count=$(echo "$metric_data" | awk '{print $1}')
    local timestamp=$(echo "$metric_data" | awk '{print $2}')
    
    # Format count as integer if it's a number
    if [[ "$object_count" != "NO_DATA" ]]; then
      object_count=${object_count%.*}
    fi
    
    printf "%s\t%s\t%s\n" "$bucket_name" "$object_count" "$timestamp"
  done | columnise
}

buckets() {

  # List S3 Buckets
  #
  #     $ buckets
  #     web-assets  2019-12-20  08:24:38.182045
  #     backups     2019-12-20  08:24:44.351215
  #     archive     2019-12-20  08:24:57.567652

  local buckets=$(skim-stdin)
  local filters=$(__bma_read_filters $@)

  aws s3api list-buckets \
    --output text \
    --query "
      Buckets[${buckets:+?contains(['${buckets// /"','"}'], Name)}].[
        Name,
        CreationDate
      ]" |
    grep -E -- "$filters" |
    column -t
}

bucket-acls() {

  # List S3 Bucket Access Control Lists.
  #
  #     $ bucket-acls another-example-bucket
  #     another-example-bucket
  #
  # !!! Note
  #     The only recommended use case for the bucket ACL is to grant write
  #     permission to the Amazon S3 Log Delivery group to write access log
  #     objects to your bucket. [AWS docs](https://docs.aws.amazon.com/AmazonS3/latest/dev/access-policy-alternatives-guidelines.html)

  local buckets=$(skim-stdin "$@")
  [[ -z "$buckets" ]] && __bma_usage "bucket [bucket]" && return 1

  local bucket
  for bucket in $buckets; do
    aws s3api get-bucket-acl \
      --bucket "$bucket" \
      --output text \
      --query "[
        '$bucket',
        join(' ', Grants[?Grantee.Type=='Group'].[join('=',[Permission, Grantee.URI])][])
      ]" |
      sed 's#http://acs.amazonaws.com/groups/##g'
  done
}

bucket-remove() {

  # Remove an empty S3 Bucket.
  #
  # *In this example the bucket is not empty.*
  #
  #     $ bucket-remove another-example-bucket
  #     You are about to remove the following buckets:
  #     another-example-bucket  2019-12-07  06:51:12.022496
  #     Are you sure you want to continue? y
  #     remove_bucket failed: s3://another-example-bucket An error occurred (BucketNotEmpty) when calling the DeleteBucket operation: The bucket you tried to delete is not empty

  local buckets=$(skim-stdin "$@")
  [[ -z "$buckets" ]] && __bma_usage "bucket [bucket]" && return 1

  echo "You are about to remove the following buckets:"
  echo "$buckets" | tr ' ' "\n" | buckets
  [ -t 0 ] || exec </dev/tty # reattach keyboard to STDIN
  local regex_yes="^[Yy]$"
  read -p "Are you sure you want to continue? " -n 1 -r
  echo
  if [[ $REPLY =~ $regex_yes ]]; then
    local bucket
    for bucket in $buckets; do
      aws s3 rb "s3://${bucket}"
    done
  fi
}

bucket-remove-force() {

  # Remove an S3 Bucket, and delete all objects if it's not empty.
  #
  #     $ bucket-remove-force another-example-bucket
  #     You are about to delete all objects from and remove the following buckets:
  #     another-example-bucket  2019-12-07  06:51:12.022496
  #     Are you sure you want to continue? y
  #     delete: s3://another-example-bucket/aliases
  #     remove_bucket: another-example-bucket

  local buckets=$(skim-stdin "$@")
  [[ -z "$buckets" ]] && __bma_usage "bucket [bucket]" && return 1

  echo "You are about to delete all objects from and remove the following buckets:"
  echo "$buckets" | tr ' ' "\n" | buckets
  [ -t 0 ] || exec </dev/tty # reattach keyboard to STDIN
  local regex_yes="^[Yy]$"
  read -p "Are you sure you want to continue? " -n 1 -r
  echo
  if [[ $REPLY =~ $regex_yes ]]; then
    local bucket
    for bucket in $buckets; do
      aws s3 rb --force "s3://${bucket}"
    done
  fi
}

bucket-size() {
  # List S3 bucket sizes by storage class using CloudWatch metrics
  #
  # USAGE: bucket-size [--all] [bucket-name] [bucket-name]
  #        echo [bucket-name] | bucket-size [--all]
  #
  # OPTIONS:
  #   --all    Show all storage classes (slower but more comprehensive)
  #
  # EXAMPLES:
  #     $ bucket-size my-bucket
  #     my-bucket  STANDARD=15.4GB  STANDARD_IA=0B  GLACIER=2.1GB  DEEP_ARCHIVE=0B
  #
  #     $ bucket-size --all my-bucket
  #     my-bucket  STANDARD=15.4GB  INTELLIGENT_TIERING_FA=0B  INTELLIGENT_TIERING_IA=0B  GLACIER=2.1GB  ...
  #
  #     $ buckets | bucket-size
  #     my-bucket1  STANDARD=15.4GB  STANDARD_IA=0B  GLACIER=2.1GB  DEEP_ARCHIVE=0B
  #     my-bucket2  STANDARD=1.2GB   STANDARD_IA=0B  GLACIER=0B     DEEP_ARCHIVE=0B
  #
  # NOTE: Press CTRL-C to exit early when processing multiple buckets.

  # Process arguments
  local show_all=false
  local args=()

  for arg in "$@"; do
    if [[ "$arg" == "--all" ]]; then
      show_all=true
    else
      args+=("$arg")
    fi
  done

  # Get bucket names from arguments or stdin
  local bucket_names=$(skim-stdin "${args[@]}")
  [[ -z "$bucket_names" ]] && __bma_usage "[--all] bucket-name [bucket-name]" && return 1

  # Set up trap for CTRL-C with cleaner handling
  local trap_set="true"
  trap 'echo ""; echo "Interrupted by user"; trap - INT; kill $(jobs -p) 2>/dev/null; return 1' INT

  # Define storage types based on --all flag
  local storage_types

  # Common storage types (default)
  local common_storage_types=(
    "StandardStorage"
    "StandardIAStorage"
    "GlacierStorage"
    "DeepArchiveStorage"
  )

  # All storage types
  local all_storage_types=(
    "StandardStorage"
    "IntelligentTieringFAStorage"
    "IntelligentTieringIAStorage"
    "IntelligentTieringAAStorage"
    "IntelligentTieringAIAStorage"
    "IntelligentTieringDAAStorage"
    "StandardIAStorage"
    "StandardIASizeOverhead"
    "OneZoneIAStorage"
    "OneZoneIASizeOverhead"
    "ReducedRedundancyStorage"
    "GlacierStorage"
    "GlacierStagingStorage"
    "GlacierObjectOverhead"
    "GlacierS3ObjectOverhead"
    "DeepArchiveStorage"
    "DeepArchiveObjectOverhead"
    "DeepArchiveS3ObjectOverhead"
  )

  # Set storage types based on flag
  if [[ "$show_all" == "true" ]]; then
    storage_types=("${all_storage_types[@]}")
  else
    storage_types=("${common_storage_types[@]}")
  fi

  # Display names for storage types
  declare -A display_names=(
    ["StandardStorage"]="STANDARD"
    ["IntelligentTieringFAStorage"]="INTELLIGENT_TIERING_FA"
    ["IntelligentTieringIAStorage"]="INTELLIGENT_TIERING_IA"
    ["IntelligentTieringAAStorage"]="INTELLIGENT_TIERING_AA"
    ["IntelligentTieringAIAStorage"]="INTELLIGENT_TIERING_AIA"
    ["IntelligentTieringDAAStorage"]="INTELLIGENT_TIERING_DAA"
    ["StandardIAStorage"]="STANDARD_IA"
    ["StandardIASizeOverhead"]="STANDARD_IA_OVERHEAD"
    ["OneZoneIAStorage"]="ONEZONE_IA"
    ["OneZoneIASizeOverhead"]="ONEZONE_IA_OVERHEAD"
    ["ReducedRedundancyStorage"]="RRS"
    ["GlacierStorage"]="GLACIER"
    ["GlacierStagingStorage"]="GLACIER_STAGING"
    ["GlacierObjectOverhead"]="GLACIER_OVERHEAD"
    ["GlacierS3ObjectOverhead"]="GLACIER_S3_OVERHEAD"
    ["DeepArchiveStorage"]="DEEP_ARCHIVE"
    ["DeepArchiveObjectOverhead"]="DEEP_ARCHIVE_OVERHEAD"
    ["DeepArchiveS3ObjectOverhead"]="DEEP_ARCHIVE_S3_OVERHEAD"
  )

  # Function to format size in human-readable format
  format_size() {
    local bytes=$1

    # Use awk for all comparisons to handle floating point numbers properly
    awk -v bytes="$bytes" '
    BEGIN {
      if (bytes == 0) {
        print "0B"
      } else if (bytes < 1024) {
        printf "%.0fB", bytes
      } else if (bytes < 1048576) {
        printf "%.1fKB", bytes/1024
      } else if (bytes < 1073741824) {
        printf "%.1fMB", bytes/1048576
      } else if (bytes < 1099511627776) {
        printf "%.1fGB", bytes/1073741824
      } else if (bytes < 1125899906842624) {
        printf "%.1fTB", bytes/1099511627776
      } else {
        printf "%.1fPB", bytes/1125899906842624
      }
    }'
  }

  # Create a temporary directory for results
  local tmp_dir=$(mktemp -d)

  # Process each bucket
  local bucket_name
  for bucket_name in $bucket_names; do
    # Get the start and end time once for all queries
    local start_time=$(date -u -d '2 days ago' '+%Y-%m-%dT%H:%M:%SZ')
    local end_time=$(date -u '+%Y-%m-%dT%H:%M:%SZ')

    # Create a directory for this bucket's results
    mkdir -p "${tmp_dir}/${bucket_name}"

    # Launch parallel requests for each storage type
    for storage_type in "${storage_types[@]}"; do
      (
        # Get metrics for this storage type
        aws cloudwatch get-metric-statistics \
          --namespace AWS/S3 \
          --metric-name BucketSizeBytes \
          --dimensions Name=BucketName,Value="$bucket_name" Name=StorageType,Value="$storage_type" \
          --start-time "$start_time" \
          --end-time "$end_time" \
          --period 86400 \
          --statistics Average \
          --output json >"${tmp_dir}/${bucket_name}/${storage_type}.json"
      ) &

      # Limit the number of parallel processes to avoid overwhelming the system
      # Adjust this number based on your system's capabilities
      if [[ $(jobs -r | wc -l) -ge 10 ]]; then
        wait -n
      fi
    done

    # Wait for all background jobs to complete for this bucket
    wait

    # Process the results
    local output="$bucket_name"
    local has_data=false

    for storage_type in "${storage_types[@]}"; do
      local metric_data_file="${tmp_dir}/${bucket_name}/${storage_type}.json"

      if [[ -f "$metric_data_file" ]]; then
        # Extract and format the size
        local size=0
        local datapoints_count=$(jq '.Datapoints | length' <"$metric_data_file")

        if [[ $datapoints_count -gt 0 ]]; then
          size=$(jq -r '.Datapoints[0].Average' <"$metric_data_file")
        fi

        local formatted_size=$(format_size $size)

        # Determine if we should show this storage type
        # - Always show common storage types
        # - For all storage types, only show non-zero ones (except STANDARD)
        if [[ "$show_all" == "false" ]] || [[ "$size" != "0" ]] || [[ "$storage_type" == "StandardStorage" ]]; then
          display_name="${display_names[$storage_type]}"
          output="$output"$'\t'"$display_name=$formatted_size"
          has_data=true
        fi
      fi
    done

    # If no data was found, at least show STANDARD=0B
    if [[ "$has_data" == "false" ]]; then
      output="$output"$'\t'"STANDARD=0B"
    fi

    echo "$output"
  done | columnise

  # Clean up temporary files
  rm -rf "$tmp_dir"

  # Reset the trap when we're done
  trap - INT
}

bucket-inventory-destination() {

  # Show where inventory is written for S3 buckets
  #
  #     $ bucket-inventory-destination my-bucket
  #     my-bucket  inventory-destination-bucket  reports/my-bucket/
  #
  #     $ buckets | bucket-inventory-destination
  #     bucket-1  inventory-bucket  reports/bucket-1/
  #     bucket-2  NO_INVENTORY      NO_INVENTORY

  local buckets=$(skim-stdin "$@")
  [[ -z "$buckets" ]] && __bma_usage "bucket [bucket]" && return 1

  local bucket
  for bucket in $buckets; do
    local result=$(aws s3api list-bucket-inventory-configurations \
      --bucket "$bucket" \
      --output text \
      --query "
        InventoryConfigurationList[0].[
          '$bucket',
          Destination.S3BucketDestination.Bucket,
          Destination.S3BucketDestination.Prefix
        ]" 2>/dev/null)

    # If result is empty or None, bucket has no inventory configuration
    if [[ -z "$result" || "$result" == "None" ]]; then
      echo -e "$bucket\tNO_INVENTORY\tNO_INVENTORY"
    else
      # Handle the case where prefix might be empty (tab-separated values)
      echo "$result" | awk -F'\t' '{
        OFS="\t"
        if ($2 == "" || $2 == "None") {
          print $1, "NO_INVENTORY", "NO_INVENTORY"
        } else if ($3 == "") {
          print $1, $2, "(root)"
        } else {
          print $0
        }
      }'
    fi
  done |
    columnise
}

bucket-inventory-status() {

  # Show inventory configuration status for S3 buckets
  #
  #     $ bucket-inventory-status my-bucket
  #     my-bucket  ENABLED   daily    ORC       my-inventory-id
  #
  #     $ buckets | bucket-inventory-status
  #     bucket-1  ENABLED   daily    Parquet   inventory-config-1
  #     bucket-2  DISABLED  NO_CONFIG NO_CONFIG NO_CONFIG

  local buckets=$(skim-stdin "$@")
  [[ -z "$buckets" ]] && __bma_usage "bucket [bucket]" && return 1

  local bucket
  for bucket in $buckets; do
    local result=$(aws s3api list-bucket-inventory-configurations \
      --bucket "$bucket" \
      --output text \
      --query "
        InventoryConfigurationList[0].[
          '$bucket',
          (IsEnabled && 'ENABLED') || 'DISABLED',
          Schedule.Frequency || 'NO_CONFIG',
          Destination.S3BucketDestination.Format || 'NO_CONFIG',
          Id || 'NO_CONFIG'
        ]" 2>/dev/null)

    # If result is empty or None, bucket has no inventory configuration
    if [[ -z "$result" || "$result" == "None" ]]; then
      echo -e "$bucket\tNOT_CONFIGURED\tNOT_CONFIGURED\tNOT_CONFIGURED\tNOT_CONFIGURED"
    else
      echo "$result"
    fi
  done |
    columnise

  set +x
}

bucket-lock-configuration() {

  # Show object lock configuration for S3 buckets
  #
  #     $ bucket-lock-configuration my-bucket
  #     my-bucket  ENABLED  COMPLIANCE  30  Days
  #
  #     $ buckets | bucket-lock-configuration
  #     bucket-1  ENABLED   GOVERNANCE  1   Years
  #     bucket-2  DISABLED  NO_CONFIG   NO_CONFIG  NO_CONFIG

  local buckets=$(skim-stdin "$@")
  [[ -z "$buckets" ]] && __bma_usage "bucket [bucket]" && return 1

  local bucket
  for bucket in $buckets; do
    aws s3api get-object-lock-configuration \
      --bucket "$bucket" \
      --output text \
      --query "
        ObjectLockConfiguration.[
          '$bucket',
          (ObjectLockEnabled == 'Enabled' && 'ENABLED') || 'DISABLED',
          Rule.DefaultRetention.Mode || 'NO_CONFIG',
          Rule.DefaultRetention.Days || Rule.DefaultRetention.Years || 'NO_CONFIG',
          (Rule.DefaultRetention.Days && 'Days') || (Rule.DefaultRetention.Years && 'Years') || 'NO_CONFIG'
        ]" 2>/dev/null || echo -e "$bucket\tDISABLED\tNO_CONFIG\tNO_CONFIG\tNO_CONFIG"
  done |
    columnise
}

bucket-lock-stats() {

  # Show object lock statistics for S3 buckets using CloudWatch metrics
  #
  #     $ bucket-lock-stats my-bucket
  #     my-bucket  1234567  567890   666677
  #
  # COLUMNS:
  #     BUCKET_NAME  TOTAL_OBJECTS  LOCKED_OBJECTS  UNLOCKED_OBJECTS

  local buckets=$(skim-stdin "$@")
  [[ -z "$buckets" ]] && __bma_usage "bucket [bucket]" && return 1

  # Get time range (last 2 days to ensure we get recent data)
  local start_time=$(date -u -d '2 days ago' '+%Y-%m-%dT%H:%M:%SZ')
  local end_time=$(date -u '+%Y-%m-%dT%H:%M:%SZ')

  local bucket
  for bucket in $buckets; do
    # Get total number of objects
    local total_objects=$(aws cloudwatch get-metric-statistics \
      --namespace AWS/S3 \
      --metric-name NumberOfObjects \
      --dimensions Name=BucketName,Value="$bucket" Name=StorageType,Value=AllStorageTypes \
      --start-time "$start_time" \
      --end-time "$end_time" \
      --period 86400 \
      --statistics Average \
      --output text \
      --query 'Datapoints[0].Average' 2>/dev/null || echo "0")

    # Note: AWS doesn't provide direct metrics for locked vs unlocked objects
    # This would need to be calculated from inventory data or S3 API calls
    echo -e "$bucket\t${total_objects%.*}\tCHECK_INVENTORY\tCHECK_INVENTORY"
  done |
    columnise
}

bucket-objects() {
  # List objects in S3 buckets with safety checks for large buckets
  #
  # XXX: Non-standard use of --prefix/--max-keys/--no-prompt arguments for bash-my-aws
  #
  #     $ bucket-objects my-bucket
  #     data/file1.txt     2025-06-30T10:00:00.000Z  1024      STANDARD
  #     data/file2.json    2025-06-29T15:30:00.000Z  2048576   GLACIER
  #
  #     $ bucket-objects --prefix data/ my-bucket
  #     data/file1.txt     2025-06-30T10:00:00.000Z  1024      STANDARD
  #     data/file2.json    2025-06-29T15:30:00.000Z  2048576   GLACIER
  #
  # OPTIONS:
  #   --prefix PREFIX    List only objects with this prefix
  #   --max-keys NUMBER  Maximum objects per page (default: 1000)
  #   --no-prompt        Don't prompt for confirmation on large buckets
  #
  # COLUMNS:
  #   OBJECT_KEY  LAST_MODIFIED  SIZE_BYTES  STORAGE_CLASS

  # Process arguments
  local prefix=""
  local max_keys=1000
  local no_prompt=false
  local args=()

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --prefix)
        prefix="$2"
        shift 2
        ;;
      --max-keys)
        max_keys="$2"
        shift 2
        ;;
      --no-prompt)
        no_prompt=true
        shift
        ;;
      *)
        args+=("$1")
        shift
        ;;
    esac
  done

  # Get bucket names
  local bucket_names=$(skim-stdin "${args[@]}")
  [[ -z "$bucket_names" ]] && __bma_usage "[--prefix PREFIX] [--max-keys NUMBER] [--no-prompt] bucket-name [bucket-name]" && return 1

  local bucket_name
  for bucket_name in $bucket_names; do
    # First check object count if not bypassing prompt
    if [[ "$no_prompt" != "true" ]]; then
      echo "Checking object count for $bucket_name..." >&2
      
      # Use bucket-object-count function to get the count
      local count_output=$(bucket-object-count "$bucket_name" | grep -w "^$bucket_name")
      local object_count=$(echo "$count_output" | cut -f2)
      
      # Handle NO_DATA case
      if [[ "$object_count" == "NO_DATA" ]]; then
        echo "Warning: Unable to determine object count from CloudWatch metrics" >&2
        object_count="0"
      fi
      
      # Prompt if more than 1000 objects (and we have a valid count)
      if [[ "$object_count" =~ ^[0-9]+$ ]] && [[ "$object_count" -gt 1000 ]]; then
        echo "" >&2
        echo "WARNING: Bucket '$bucket_name' contains approximately $object_count objects." >&2
        echo "Listing all objects may take time and incur API costs." >&2
        [ -t 0 ] || exec </dev/tty # reattach keyboard to STDIN
        local regex_yes="^[Yy]$"
        read -p "Continue? (y/N) " -n 1 -r
        echo >&2
        if [[ ! $REPLY =~ $regex_yes ]]; then
          echo "Skipping $bucket_name" >&2
          continue
        fi
      fi
    fi

    # List objects with pagination
    local continuation_token=""
    local page_count=0
    local total_listed=0
    
    while true; do
      # Build AWS CLI command
      local aws_cmd=(
        aws s3api list-objects-v2
        --bucket "$bucket_name"
        --max-keys "$max_keys"
        --output text
        --query "Contents[].[Key, LastModified, Size, StorageClass || 'STANDARD']"
      )
      
      # Add optional parameters
      [[ -n "$prefix" ]] && aws_cmd+=(--prefix "$prefix")
      [[ -n "$continuation_token" ]] && aws_cmd+=(--continuation-token "$continuation_token")
      
      # Execute and capture output
      local output
      local cmd_result
      output=$("${aws_cmd[@]}" 2>&1)
      cmd_result=$?
      
      if [[ $cmd_result -ne 0 ]]; then
        echo "Error listing objects in $bucket_name: $output" >&2
        break
      fi
      
      # Output the results
      if [[ -n "$output" ]]; then
        echo "$output"
        total_listed=$((total_listed + $(echo "$output" | wc -l)))
      fi
      
      # Check if there are more objects
      continuation_token=$(aws s3api list-objects-v2 \
        --bucket "$bucket_name" \
        ${prefix:+--prefix "$prefix"} \
        --max-keys "$max_keys" \
        ${continuation_token:+--continuation-token "$continuation_token"} \
        --output text \
        --query "NextContinuationToken" 2>/dev/null)
      
      # Break if no more pages
      [[ -z "$continuation_token" ]] || [[ "$continuation_token" == "None" ]] && break
      
      # Increment page count
      page_count=$((page_count + 1))
      
      # Prompt for continuation every 10 pages (10,000 objects by default)
      if [[ $((page_count % 10)) -eq 0 ]] && [[ "$no_prompt" != "true" ]]; then
        echo "" >&2
        echo "Listed $total_listed objects so far..." >&2
        [ -t 0 ] || exec </dev/tty # reattach keyboard to STDIN
        local regex_yes="^[Yy]$"
        read -p "Continue listing? (y/N) " -n 1 -r
        echo >&2
        if [[ ! $REPLY =~ $regex_yes ]]; then
          echo "Stopped listing at $total_listed objects" >&2
          break
        fi
      fi
    done
  done | columnise
}
